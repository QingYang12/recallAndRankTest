# æ¨èç³»ç»Ÿæ•°æ®å‡†å¤‡å®Œæ•´æ•™ç¨‹

## ğŸ“Š æ–°é—»æ¨èç³»ç»Ÿæ•°æ®å‡†å¤‡æµç¨‹

æ¨èç³»ç»Ÿæ•°æ®å‡†å¤‡ = **æ”¶é›†ç”¨æˆ·è¡Œä¸º** + **æå–ç‰¹å¾** + **æ„é€ æ ‡ç­¾** + **è´Ÿé‡‡æ ·** â†’ æ¨¡å‹è®­ç»ƒ

---

## 1ï¸âƒ£ æ•°æ®æ”¶é›†ï¼ˆå¤šæºæ•°æ®ï¼‰

### ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼ˆæœ€é‡è¦ï¼‰

```python
# ç”¨æˆ·äº¤äº’æ•°æ®
{
    "user_id": "123456",
    "news_id": "976998763",
    "action": "click",        # ç‚¹å‡»/é˜…è¯»/åˆ†äº«/æ”¶è—/ç‚¹èµ/è¯„è®º
    "duration": 45,           # é˜…è¯»æ—¶é•¿ï¼ˆç§’ï¼‰
    "timestamp": 1770365380,
    "device": "iOS",
    "channel": "æ¨èæµ"
}
```

**å…³é”®è¡Œä¸º**ï¼š
- âœ… **æ­£æ ·æœ¬**ï¼šç‚¹å‡»ã€é˜…è¯»å®Œæ•´ã€åˆ†äº«ã€æ”¶è—ã€ç‚¹èµ
- âŒ **è´Ÿæ ·æœ¬**ï¼šæ›å…‰æœªç‚¹å‡»ã€å¿«é€Ÿè·³å‡ºï¼ˆ<3ç§’ï¼‰ã€ä¸æ„Ÿå…´è¶£
- âš ï¸ **éšå¼åé¦ˆ**ï¼šåœç•™æ—¶é•¿ã€æ»‘åŠ¨é€Ÿåº¦ã€é‡å¤è®¿é—®

### æ–°é—»å†…å®¹æ•°æ®

```python
{
    "news_id": "976998763",
    "title": "æ ‡é¢˜",
    "content": "æ­£æ–‡å†…å®¹",
    "category": "ç§‘æŠ€",
    "tags": ["AI", "æœºå™¨å­¦ä¹ "],
    "author": "ä½œè€…ID",
    "publish_time": 1770365380,
    "image_url": "å°é¢å›¾",
    "source": "æ¥æºåª’ä½“"
}
```

### ç”¨æˆ·ç”»åƒæ•°æ®

```python
{
    "user_id": "123456",
    "age": 28,
    "gender": "M",
    "location": "åŒ—äº¬",
    "interests": ["ç§‘æŠ€", "è´¢ç»"],
    "reading_history": [...],
    "avg_session_duration": 600,
    "active_time": ["9:00-10:00", "20:00-22:00"]
}
```

---

## 2ï¸âƒ£ ç‰¹å¾å·¥ç¨‹ï¼ˆæ‰“æ ‡ç­¾ = ç‰¹å¾æå–ï¼‰

### ç”¨æˆ·ç‰¹å¾

```python
user_features = {
    # ç»Ÿè®¡ç‰¹å¾
    "total_clicks": 1234,
    "avg_reading_time": 45.6,
    "click_rate": 0.15,
    
    # å…´è¶£åå¥½ï¼ˆTopK åˆ†ç±»ï¼‰
    "top_categories": ["ç§‘æŠ€", "è´¢ç»", "ä½“è‚²"],
    "top_tags": ["AI", "åŒºå—é“¾", "NBA"],
    
    # æ—¶åºç‰¹å¾
    "morning_active": 0.8,      # æ—©ä¸Šæ´»è·ƒåº¦
    "evening_active": 0.6,
    
    # Embedding
    "user_embedding": [0.12, -0.45, 0.78, ...]  # ç”¨æˆ·å‘é‡
}
```

### æ–°é—»ç‰¹å¾

```python
news_features = {
    # æ–‡æœ¬ç‰¹å¾
    "title_length": 25,
    "content_length": 1500,
    "keyword_tfidf": [...],
    
    # çƒ­åº¦ç‰¹å¾
    "view_count": 5000,
    "click_rate": 0.08,
    "share_count": 200,
    "freshness": 0.95,  # æ–°é²œåº¦
    
    # Embedding
    "title_embedding": [...],    # æ ‡é¢˜å‘é‡
    "content_embedding": [...]   # å†…å®¹å‘é‡
}
```

### äº¤å‰ç‰¹å¾

```python
interaction_features = {
    "user_category_match": 0.85,  # ç”¨æˆ·å…´è¶£ä¸æ–°é—»åˆ†ç±»åŒ¹é…åº¦
    "user_tag_overlap": 3,        # æ ‡ç­¾é‡å æ•°
    "time_match": 0.9,            # æ—¶é—´åŒ¹é…ï¼ˆç”¨æˆ·æ´»è·ƒæ—¶é—´ vs æ–°é—»å‘å¸ƒæ—¶é—´ï¼‰
}
```

---

## 3ï¸âƒ£ æ ‡ç­¾æ„é€ ï¼ˆLabelï¼‰

### æ–¹å¼1ï¼šæ˜¾å¼æ ‡ç­¾ï¼ˆæ¨èï¼‰

```python
# æ­£æ ·æœ¬ï¼ˆlabel=1ï¼‰
- ç‚¹å‡»ä¸”é˜…è¯»æ—¶é•¿ > 30ç§’
- åˆ†äº«/æ”¶è—/ç‚¹èµ
- è¯„è®º

# è´Ÿæ ·æœ¬ï¼ˆlabel=0ï¼‰
- æ›å…‰ä½†æœªç‚¹å‡»
- ç‚¹å‡»ä½†é˜…è¯»æ—¶é•¿ < 3ç§’
- ç‚¹äº†"ä¸æ„Ÿå…´è¶£"
```

### æ–¹å¼2ï¼šå¤šçº§æ ‡ç­¾ï¼ˆæ›´ç²¾ç»†ï¼‰

```python
label_mapping = {
    0: "æœªæ›å…‰",
    1: "æ›å…‰æœªç‚¹å‡»",
    2: "ç‚¹å‡»ä½†å¿«é€Ÿé€€å‡º",
    3: "ç‚¹å‡»ä¸”æµè§ˆ",
    4: "æ·±åº¦é˜…è¯»ï¼ˆ>60ç§’ï¼‰",
    5: "äº’åŠ¨ï¼ˆç‚¹èµ/è¯„è®º/åˆ†äº«ï¼‰"
}
```

### æ–¹å¼3ï¼šè¿ç»­å€¼æ ‡ç­¾ï¼ˆå›å½’ï¼‰

```python
label = (
    0.4 * (reading_time / 60) +      # é˜…è¯»æ—¶é•¿å æ¯”
    0.3 * is_click +                  # æ˜¯å¦ç‚¹å‡»
    0.2 * is_share +                  # æ˜¯å¦åˆ†äº«
    0.1 * is_like                     # æ˜¯å¦ç‚¹èµ
)  # èŒƒå›´: 0-1
```

---

## 4ï¸âƒ£ è®­ç»ƒæ•°æ®æ ¼å¼

### å¬å›æ¨¡å‹ï¼ˆåŒå¡”æ¨¡å‹ï¼‰

```python
# ç”¨æˆ·å¡”
user_input = {
    "user_id": 123456,
    "user_features": [...],
    "history_items": [976998763, 976835327, ...]  # æœ€è¿‘æµè§ˆ
}

# ç‰©å“å¡”
item_input = {
    "news_id": 976998763,
    "item_features": [...]
}

# æ ‡ç­¾
label = 1  # 0/1 äºŒåˆ†ç±»
```

### æ’åºæ¨¡å‹ï¼ˆç²¾æ’ï¼‰

```python
# Wide & Deep / DeepFM ç­‰
training_sample = {
    "user_id": 123456,
    "news_id": 976998763,
    "user_features": [...],
    "item_features": [...],
    "cross_features": [...],
    "context_features": {
        "time": "morning",
        "device": "iOS",
        "location": "åŒ—äº¬"
    },
    "label": 1
}
```

---

## 5ï¸âƒ£ æ•°æ®å‡†å¤‡ä»£ç ç¤ºä¾‹

```python
import pandas as pd
from datetime import datetime, timedelta

# 1. æ”¶é›†ç”¨æˆ·è¡Œä¸ºæ•°æ®
def collect_user_behavior(days=30):
    """æ”¶é›†æœ€è¿‘30å¤©çš„ç”¨æˆ·è¡Œä¸º"""
    query = f"""
    SELECT 
        user_id,
        news_id,
        action,
        reading_duration,
        timestamp
    FROM user_behavior_log
    WHERE timestamp > {datetime.now() - timedelta(days=days)}
    """
    return pd.read_sql(query, db_conn)

# 2. æ„é€ æ­£è´Ÿæ ·æœ¬
def build_samples(behavior_df):
    # æ­£æ ·æœ¬ï¼šç‚¹å‡»ä¸”é˜…è¯»æ—¶é•¿ > 30ç§’
    positive = behavior_df[
        (behavior_df['action'] == 'click') & 
        (behavior_df['reading_duration'] > 30)
    ].copy()
    positive['label'] = 1
    
    # è´Ÿæ ·æœ¬ï¼šæ›å…‰æœªç‚¹å‡»ï¼ˆéšæœºé‡‡æ ·ï¼Œä¿æŒå¹³è¡¡ï¼‰
    negative = behavior_df[
        behavior_df['action'] == 'impression'
    ].sample(n=len(positive))
    negative['label'] = 0
    
    return pd.concat([positive, negative])

# 3. ç‰¹å¾å·¥ç¨‹
def extract_features(user_id, news_id):
    # ç”¨æˆ·ç‰¹å¾
    user_feat = get_user_features(user_id)
    
    # æ–°é—»ç‰¹å¾
    news_feat = get_news_features(news_id)
    
    # äº¤å‰ç‰¹å¾
    cross_feat = {
        'user_category_match': calculate_match(
            user_feat['top_categories'], 
            news_feat['category']
        )
    }
    
    return {**user_feat, **news_feat, **cross_feat}

# 4. ç”Ÿæˆè®­ç»ƒæ•°æ®
def generate_training_data():
    behavior_df = collect_user_behavior(days=30)
    samples = build_samples(behavior_df)
    
    training_data = []
    for _, row in samples.iterrows():
        features = extract_features(row['user_id'], row['news_id'])
        training_data.append({
            **features,
            'label': row['label']
        })
    
    return pd.DataFrame(training_data)
```

---

## 6ï¸âƒ£ å¾®è°ƒè®­ç»ƒæµç¨‹

```python
# ä½¿ç”¨ PyTorch / TensorFlow è®­ç»ƒå¬å›æ¨¡å‹
from transformers import AutoModel, AutoTokenizer

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = AutoModel.from_pretrained("bert-base-chinese")

# 2. å‡†å¤‡æ•°æ®
train_dataset = NewsDataset(training_data, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=128)

# 3. å¾®è°ƒè®­ç»ƒ
for epoch in range(epochs):
    for batch in train_loader:
        user_emb = user_tower(batch['user_features'])
        item_emb = item_tower(batch['item_features'])
        
        # ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = cosine_similarity(user_emb, item_emb)
        
        # è®¡ç®—æŸå¤±
        loss = bce_loss(similarity, batch['label'])
        loss.backward()
        optimizer.step()
```

---

## ğŸ’¡ å…³é”®è¦ç‚¹

### âœ… æœ€ä½³å®è·µ

1. **æ ·æœ¬å¹³è¡¡**ï¼šæ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ 1:1 æˆ– 1:3
2. **æ—¶é—´çª—å£**ï¼šä½¿ç”¨æœ€è¿‘ 7-30 å¤©æ•°æ®
3. **ç‰¹å¾å½’ä¸€åŒ–**ï¼šç»Ÿä¸€ç‰¹å¾é‡çº²
4. **è´Ÿé‡‡æ ·**ï¼šéšæœº + å›°éš¾è´Ÿæ ·æœ¬ï¼ˆæ›å…‰æœªç‚¹å‡»ï¼‰
5. **å®šæœŸæ›´æ–°**ï¼šæ¯å¤©/æ¯å‘¨å¢é‡è®­ç»ƒ

### âš ï¸ å¸¸è§é—®é¢˜

- **å†·å¯åŠ¨**ï¼šæ–°ç”¨æˆ·/æ–°å†…å®¹ç¼ºå°‘å†å²æ•°æ® â†’ ä½¿ç”¨äººå£ç»Ÿè®¡å­¦ç‰¹å¾
- **é©¬å¤ªæ•ˆåº”**ï¼šçƒ­é—¨å†…å®¹è¿‡åº¦æ›å…‰ â†’ æ¢ç´¢ vs åˆ©ç”¨ï¼ˆExplore & Exploitï¼‰
- **æ•°æ®åå·®**ï¼šåªè®°å½•æ­£æ ·æœ¬ â†’ éœ€è¦è®°å½•æ›å…‰æ•°æ®

---

## ğŸ¯ æ€»ç»“

æ¨èç³»ç»Ÿæ•°æ®å‡†å¤‡çš„æ ¸å¿ƒæ­¥éª¤ï¼š

1. **æ”¶é›†æ•°æ®**ï¼šç”¨æˆ·è¡Œä¸º + å†…å®¹ä¿¡æ¯ + ç”¨æˆ·ç”»åƒ
2. **ç‰¹å¾å·¥ç¨‹**ï¼šæå–ç”¨æˆ·/ç‰©å“/äº¤å‰ç‰¹å¾
3. **æ„é€ æ ‡ç­¾**ï¼šå®šä¹‰æ­£è´Ÿæ ·æœ¬è§„åˆ™
4. **è´Ÿé‡‡æ ·**ï¼šä¿æŒæ ·æœ¬å¹³è¡¡
5. **è®­ç»ƒå¾®è°ƒ**ï¼šä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹

**å…³é”®**ï¼šæ•°æ®è´¨é‡ > æ¨¡å‹å¤æ‚åº¦ï¼é«˜è´¨é‡çš„æ ‡ç­¾å’Œç‰¹å¾æ¯”å¤æ‚çš„æ¨¡å‹æ¶æ„æ›´é‡è¦ã€‚

---

## 7ï¸âƒ£ ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹

### æ–¹å¼1ï¼šåœ¨çº¿å®æ—¶é¢„æµ‹ï¼ˆæ’åºï¼‰

å¯¹å€™é€‰æ–°é—»è¿›è¡Œç²¾æ’ï¼Œè¿”å› Top-N æ¨èç»“æœã€‚

```python
def predict_for_user(user_id, candidate_news_ids):
    """ä¸ºæŒ‡å®šç”¨æˆ·å¯¹å€™é€‰æ–°é—»è¿›è¡Œæ‰“åˆ†æ’åº"""
    # 1. è·å–ç”¨æˆ·ç‰¹å¾å’Œ embedding
    user_features = get_user_features(user_id)
    user_tensor = prepare_user_input(user_features)
    
    with torch.no_grad():
        user_emb = user_tower(user_tensor)
    
    # 2. æ‰¹é‡è·å–å€™é€‰æ–°é—»çš„ embedding
    news_embeddings = []
    for news_id in candidate_news_ids:
        news_features = get_news_features(news_id)
        news_tensor = prepare_item_input(news_features)
        with torch.no_grad():
            news_emb = item_tower(news_tensor)
            news_embeddings.append(news_emb)
    
    news_embeddings = torch.stack(news_embeddings)
    
    # 3. è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†
    scores = torch.matmul(user_emb, news_embeddings.T).squeeze()
    
    # 4. æ’åº
    ranked_indices = torch.argsort(scores, descending=True)
    
    # 5. è¿”å›ç»“æœ
    results = []
    for idx in ranked_indices:
        results.append({
            'news_id': candidate_news_ids[idx],
            'score': scores[idx].item()
        })
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
recommendations = predict_for_user("123456", ["976998763", "976835327"])
```

### æ–¹å¼2ï¼šç¦»çº¿æ‰¹é‡é¢„æµ‹ï¼ˆå¬å›ï¼‰

æå‰ä¸ºæ‰€æœ‰æ–°é—»ç”Ÿæˆ embeddingï¼Œå­˜å…¥å‘é‡æ•°æ®åº“ï¼ˆFaissï¼‰ï¼Œå®ç°é«˜æ•ˆå¬å›ã€‚

```python
import faiss
import numpy as np

# æ­¥éª¤1ï¼šç¦»çº¿ç”Ÿæˆæ‰€æœ‰æ–°é—»çš„ embedding
def generate_all_news_embeddings():
    all_news_ids = get_all_news_ids()
    news_embeddings = {}
    
    for news_id in all_news_ids:
        news_features = get_news_features(news_id)
        news_tensor = prepare_item_input(news_features)
        with torch.no_grad():
            news_emb = item_tower(news_tensor).cpu().numpy()
            news_embeddings[news_id] = news_emb
    
    return news_embeddings

# æ­¥éª¤2ï¼šæ„å»º Faiss ç´¢å¼•
def build_faiss_index(news_embeddings):
    news_ids = list(news_embeddings.keys())
    embeddings = np.array([news_embeddings[nid] for nid in news_ids])
    
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ç›¸ä¼¼åº¦
    index.add(embeddings.astype('float32'))
    
    faiss.write_index(index, 'news_index.faiss')
    return index, news_ids

# æ­¥éª¤3ï¼šåœ¨çº¿å¬å›
def recall_top_k_news(user_id, top_k=100):
    # åŠ è½½ç´¢å¼•
    index = faiss.read_index('news_index.faiss')
    
    # è·å–ç”¨æˆ· embedding
    user_features = get_user_features(user_id)
    user_tensor = prepare_user_input(user_features)
    with torch.no_grad():
        user_emb = user_tower(user_tensor).cpu().numpy()
    
    # å‘é‡æ£€ç´¢
    scores, indices = index.search(user_emb.astype('float32'), top_k)
    
    return [news_ids[idx] for idx in indices[0]]
```

### æ–¹å¼3ï¼šå®Œæ•´æ¨èæµç¨‹

```python
def recommend_news_for_user(user_id, top_n=10):
    """å®Œæ•´æ¨èæµç¨‹ï¼šå¬å› â†’ æ’åº"""
    
    # 1. å¤šè·¯å¬å›
    candidates = []
    candidates.extend(recall_top_k_news(user_id, top_k=300))  # å‘é‡å¬å›
    candidates.extend(get_hot_news(limit=100))  # çƒ­é—¨å¬å›
    candidates.extend(get_category_news(user_id, limit=100))  # åˆ†ç±»å¬å›
    
    # å»é‡
    candidates = list(set(candidates))
    
    # 2. ç²¾æ’
    ranked_results = predict_for_user(user_id, candidates)
    
    # 3. è¿”å› Top-N
    return ranked_results[:top_n]

# ä½¿ç”¨
recommendations = recommend_news_for_user("123456", top_n=10)
```

### API æœåŠ¡éƒ¨ç½²

```python
from fastapi import FastAPI

app = FastAPI()

@app.on_event("startup")
async def load_models():
    global user_tower, item_tower
    user_tower = UserTower.load_from_checkpoint("user_tower.ckpt")
    item_tower = ItemTower.load_from_checkpoint("item_tower.ckpt")

@app.post("/recommend")
async def get_recommendations(user_id: str, top_n: int = 10):
    results = recommend_news_for_user(user_id, top_n)
    return {"recommendations": results}

# å¯åŠ¨: uvicorn api:app --port 8000
```

---

## ğŸš€ ä¸Šçº¿ Checklist

### éƒ¨ç½²å‰
- [ ] æ¨¡å‹æ€§èƒ½éªŒè¯ï¼ˆAUCã€NDCGï¼‰
- [ ] A/B æµ‹è¯•å‡†å¤‡
- [ ] æ€§èƒ½å‹æµ‹ï¼ˆQPSã€å»¶è¿Ÿï¼‰
- [ ] ç›‘æ§å‘Šè­¦é…ç½®

### ä¸Šçº¿å
- [ ] å®æ—¶ç›‘æ§ç‚¹å‡»ç‡
- [ ] å®šæœŸé‡è®­ç»ƒï¼ˆæ¯å¤©/æ¯å‘¨ï¼‰
- [ ] æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
- [ ] å¼‚å¸¸æ£€æµ‹

---

## ğŸ“Š æ•ˆæœè¯„ä¼°

### ç¦»çº¿æŒ‡æ ‡
```python
from sklearn.metrics import roc_auc_score, ndcg_score

# AUCï¼ˆåˆ†ç±»æ•ˆæœï¼‰
auc = roc_auc_score(y_true, y_pred)

# NDCGï¼ˆæ’åºæ•ˆæœï¼‰
ndcg = ndcg_score([y_true], [y_pred])

# è¦†ç›–ç‡ï¼ˆå¤šæ ·æ€§ï¼‰
coverage = len(set(recommended_items)) / len(all_items)
```

### åœ¨çº¿æŒ‡æ ‡
- **ç‚¹å‡»ç‡ï¼ˆCTRï¼‰**ï¼šç‚¹å‡»æ•° / æ›å…‰æ•°
- **é˜…è¯»å®Œæˆç‡**ï¼šå®Œæ•´é˜…è¯» / ç‚¹å‡»æ•°
- **åœç•™æ—¶é•¿**ï¼šå¹³å‡é˜…è¯»æ—¶é•¿
- **ç”¨æˆ·ç•™å­˜**ï¼šæ¬¡æ—¥/7æ—¥ç•™å­˜ç‡
